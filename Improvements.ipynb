{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brandon.tomlin\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\Users\\brandon.tomlin\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\qda.py:4: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n",
      "C:\\Users\\brandon.tomlin\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\matplotlib\\__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from StringIO import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import expon\n",
    "\n",
    "import scipy.stats as ss\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn import tree \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.cluster import KMeans as KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNeighborsClassifier\n",
    "from sklearn.svm import SVC as SVC\n",
    "from sklearn.svm import SVR as SVR \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB as GaussianN\n",
    "from sklearn.lda import LDA \n",
    "from sklearn.qda import QDA  \n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTreeClassifier\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#from sklearn.metrics.confusion_matrix import n\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTreeClassifier\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sknn.mlp import Classifier, Regressor, Layer\n",
    "from sknn.mlp import Classifier, Layer, MultiLayerPerceptron\n",
    "\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'data/fertility_Diagnosis.txt'\n",
    "\n",
    "names = ('Season', \n",
    "        'Age', \n",
    "        'Child_Diseases', \n",
    "        'Accidents', \n",
    "        'Surgical_Intervention',\n",
    "        'Fever', \n",
    "        'Drinking', \n",
    "        'Smoking', \n",
    "        'Hours_Sitting', \n",
    "        'target')\n",
    "\n",
    "df = pd.read_csv(path, names = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>Season</th>\n",
       "      <th>Age</th>\n",
       "      <th>Child_Diseases</th>\n",
       "      <th>Accidents</th>\n",
       "      <th>Surgical_Intervention</th>\n",
       "      <th>Fever</th>\n",
       "      <th>Drinking</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Hours_Sitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  Season   Age  Child_Diseases  Accidents  Surgical_Intervention  \\\n",
       "0       0   -0.33  0.69               0          1                      1   \n",
       "1       1   -0.33  0.94               1          0                      1   \n",
       "\n",
       "   Fever  Drinking  Smoking  Hours_Sitting  \n",
       "0      0       0.8        0           0.88  \n",
       "1      0       0.8        1           0.31  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perpareTarget(df, targetName, yes, no):   # prepares the Target column by mapping targetcolumn and moving to first col\n",
    "    from sklearn import preprocessing\n",
    "    le_dep = preprocessing.LabelEncoder()\n",
    "    #to convert into numbers\n",
    "    df[targetName] = le_dep.fit_transform(df[targetName])\n",
    "    #df[targetName] = df[targetName].map({yes: '1', no: '0'})\n",
    "    #df = df.fillna(0)\n",
    "    targetSeries = df[targetName]\n",
    "    del df[targetName]\n",
    "    df.insert(0, targetName, targetSeries)\n",
    "    df = df.rename(columns={targetName:'target'})\n",
    "    expected = targetName\n",
    "    df['target'] = df.target.astype(int)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "targetName = ('target')  \n",
    "df = perpareTarget(df, targetName, 'No', 'Yes')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setting feature testing and training\n",
    "    second instance is of scaled data.\n",
    "    Its important to note, that the dataset was already normalized before beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(df.ix[:,1:].values, \n",
    "                                                                            df.ix[:,0].values, \n",
    "                                                                            test_size=0.40, \n",
    "                                                                            random_state=0)\n",
    "\n",
    "features_X_train, features_X_test, target_y_train, target_y_test = train_test_split(df.ix[:,1:].values, \n",
    "                                                                            df.ix[:,0].values, \n",
    "                                                                            test_size=.40, \n",
    "                                                                            random_state=0)\n",
    "std_scale = preprocessing.StandardScaler().fit(features_X_train)\n",
    "X_train = std_scale.transform(features_X_train)\n",
    "X_test = std_scale.transform(features_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Possible SMOTE required\n",
    "    Synthetic Minority Over-sampling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of Target that is Yes 0.075\n"
     ]
    }
   ],
   "source": [
    "global features_test\n",
    "global features_train\n",
    "global target_test\n",
    "global target_train\n",
    "print \"Percent of Target that is Yes\", target_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusionMatrixPlot(target_test, target_predicted, target_names):  # target_test, target_predicted_MODEL\n",
    "    cm = confusion_matrix(target_test, target_predicted)\n",
    "    plt.matshow(cm)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label') \n",
    "    print ('Confusion Matrix Plot:')\n",
    "    print(cm)\n",
    "\n",
    "    ### ADD TO MODEL \n",
    "    #### scores_SGD = cross_val_score(SGD, features_train, target_train)\n",
    "\n",
    "    #### print scores_SGD\n",
    "    #### print scores_SGD.mean()\n",
    "\n",
    "    accScore = accuracy_score(target_test, \n",
    "                     target_predicted)\n",
    "    print ('Accuracy Score:')\n",
    "    print (accScore)\n",
    "\n",
    "    classReport = classification_report(target_test, \n",
    "                            target_predicted, \n",
    "                            target_names= target_names)\n",
    "\n",
    "    print('classification report:') \n",
    "    print(classReport)\n",
    "    print ('Confusion Matrix:')\n",
    "    conMatrix = confusion_matrix(target_test, \n",
    "                       target_predicted)\n",
    "    print (conMatrix)\n",
    "\n",
    "def CvScores(target_predicted, \n",
    "             features_train,  \n",
    "             target_train, \n",
    "             cv):     # target_predicted, features_train,  target_train, cv(int))\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    \n",
    "    scores_rf = cross_val_score(target_predicted,\n",
    "                                features_train, \n",
    "                                target_train, \n",
    "                                cv     = cv,\n",
    "                                scoring='roc_auc',\n",
    "                                n_jobs = -1)\n",
    "    print \"Cross Validation Score for each K\", scores_rf, '\\n'\n",
    "    scores_rf.mean()\n",
    "    print(\"ROC AUC: %0.2f (+/- %0.2f)\"  % (scores_rf.mean(), scores_rf.std()))\n",
    "\n",
    "def accScore(clf_y_train_pred, clf_y_test_pred):    \n",
    "    clf_train = accuracy_score(target_train, clf_y_train_pred) \n",
    "    clf_test  = accuracy_score(target_test, clf_y_test_pred) \n",
    "\n",
    "    print('train/test accuracies %.3f / %.3f'\n",
    "      % (clf_train, clf_test))\n",
    "\n",
    "    'roc_auc'\n",
    "    \n",
    "def gridSearch(model, parameters, cv):\n",
    "    grid = GridSearchCV(model, \n",
    "                        parameters,\n",
    "                        n_jobs = -1, \n",
    "                        cv     = cv\n",
    "                       )\n",
    "    grid.fit(features_train,\n",
    "             target_train)\n",
    "    #target_predicted_SVR = clf_linSVR.predict(features_test)\n",
    "    grid.fit(features_train, target_train)\n",
    "    \n",
    "    print \"SCORES\",     grid.grid_scores_ , '\\n'\n",
    "    print \"BEST Estm\",  grid.best_estimator_, '\\n'\n",
    "    print \"BEST SCORE\", grid.best_score_, '\\n'\n",
    "    print \"BEST PARAM\", grid.best_params_, '\\n' \n",
    "    \n",
    "#CvScores(target_predicted, features_train,  target_train, 2, 'roc_auc')    \n",
    "#confusionMatrixPlot(target_test, target_predicted_rf, target_names)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-a94bf3579044>:44: SyntaxWarning: name 'clf_y_train_pred' is assigned to before global declaration\n",
      "  global clf_y_train_pred\n",
      "<ipython-input-9-a94bf3579044>:45: SyntaxWarning: name 'clf_y_test_pred' is assigned to before global declaration\n",
      "  global clf_y_test_pred\n",
      "<ipython-input-9-a94bf3579044>:46: SyntaxWarning: name 'target_predicted' is assigned to before global declaration\n",
      "  global target_predicted\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score as accuracy_score\n",
    "from sklearn.cluster import KMeans as KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNeighborsClassifier\n",
    "from sklearn.svm import SVR as SVR \n",
    "from sklearn.svm import LinearSVC as LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB  as GaussianNB\n",
    "from sklearn.lda import LDA as LDA\n",
    "from sklearn.qda import QDA as QDA \n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier as AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier as BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier as ExtraTreesClassifier  \n",
    "from sklearn.ensemble import GradientBoostingClassifier as GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier as StochasticGradientDescent  \n",
    "\n",
    "target_names = [\"0 = no\", \"1 = yes\"]\n",
    "expected = target_test\n",
    "        \n",
    "def workerFunction(clf, features_train, features_test, target_train, target_test):\n",
    "    # Libraries \n",
    "\n",
    "    clf = clf\n",
    "    target_names = [\"0 = no\", \"1 = yes\"]\n",
    "    expected = target_test\n",
    "\n",
    "    clf.fit(features_train, target_train) \n",
    "    \n",
    "    target_predicted = clf.predict(features_test)\n",
    "    \n",
    "    clf_y_train_pred = clf.predict(features_train)\n",
    "    clf_y_test_pred = clf.predict(features_test)\n",
    "    \n",
    "    clf_train = accuracy_score(target_train, clf_y_train_pred) \n",
    "    clf_test  = accuracy_score(target_test,  clf_y_test_pred) \n",
    "    \"\"\"\n",
    "    models_To_Run\n",
    "    \n",
    "    models_To_Run()\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    global clf_y_train_pred\n",
    "    global clf_y_test_pred\n",
    "    global target_predicted\n",
    "    \n",
    "    return (clf, target_predicted, clf_y_train_pred, clf_y_test_pred)\n",
    "\n",
    "# def accScore(clf_y_train_pred, clf_y_test_pred):    \n",
    "#     clf_train = accuracy_score(target_train, clf_y_train_pred) \n",
    "#     clf_test  = accuracy_score(target_test, clf_y_test_pred) \n",
    "\n",
    "#     print('train/test accuracies %.3f / %.3f'\n",
    "#       % (clf_train, clf_test))\n",
    "    \n",
    "    \n",
    "names = [\n",
    "    'KNN', \n",
    "    'DecisionTree',\n",
    "    'DecisionTree1',\n",
    "    'RandomForest', \n",
    "    'Support Vector Machine (Linear)', \n",
    "    'ExtraTrees',\n",
    "    'GradientBoosting', \n",
    "    'StochasticGradientDescent', \n",
    "    'MLP',\n",
    "    'MLP1', \n",
    "    'MLP2', \n",
    "    'MLP3'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(n_neighbors = 4, \n",
    "                        algorithm   = 'brute', \n",
    "                        leaf_size   = 20, \n",
    "                        metric      = 'cosine'),\n",
    "    \n",
    "   DecisionTreeClassifier(criterion        = 'gini', \n",
    "                          splitter         = 'best',          \n",
    "                          max_depth        = 1,      \n",
    "                          min_samples_split= 1, \n",
    "                          min_samples_leaf = 1,      \n",
    "                          min_weight_fraction_leaf = 0.0 , \n",
    "                          max_features     = None,   \n",
    "                          random_state     = None,      \n",
    "                          max_leaf_nodes   = None,   \n",
    "                          class_weight     = None,      \n",
    "                          presort          = False),\n",
    "    \n",
    "   DecisionTreeClassifier(criterion    = 'entropy', \n",
    "                      splitter         = 'best',          \n",
    "                      max_depth        = 1,      \n",
    "                      min_samples_split= 1, \n",
    "                      min_samples_leaf = 1,      \n",
    "                      min_weight_fraction_leaf = 0.0 , \n",
    "                      max_features     = None,   \n",
    "                      random_state     = None,      \n",
    "                      max_leaf_nodes   = None,   \n",
    "                      class_weight     = None,      \n",
    "                      presort          = False),\n",
    "\n",
    "   RandomForestClassifier(n_estimators      = 500 ,      \n",
    "                          criterion         = 'gini',\n",
    "                          max_depth         = None,    \n",
    "                          min_samples_split = 2,     \n",
    "                          min_samples_leaf  = 1,       \n",
    "                          min_weight_fraction_leaf = 0.0, \n",
    "                          max_features      = 'auto',  \n",
    "                          max_leaf_nodes    = None, \n",
    "                          bootstrap         = True,    \n",
    "                          oob_score         = False, \n",
    "                          n_jobs            = -1,      \n",
    "                          random_state      = True , \n",
    "                          verbose           = 0 ,      \n",
    "                          warm_start        = False, \n",
    "                          class_weight      = None),\n",
    "    \n",
    "   LinearSVC(penalty = 'l2',  \n",
    "             loss    = 'squared_hinge', \n",
    "             dual    =  True, \n",
    "             tol     = 0.0001, \n",
    "             C       = 1.0,  \n",
    "             class_weight = 'auto'),\n",
    "\n",
    "   ExtraTreesClassifier(criterion         = 'gini',  \n",
    "                        max_depth         =  None,        \n",
    "                        min_samples_split = 2,       \n",
    "                        min_samples_leaf  = 1, \n",
    "                        min_weight_fraction_leaf = 0.0,     \n",
    "                        max_features      = 'auto',     \n",
    "                        max_leaf_nodes    = None,    \n",
    "                        random_state      = None),\n",
    "\n",
    "   GradientBoostingClassifier(loss              = 'deviance',  \n",
    "                              learning_rate     = 0.1,     \n",
    "                              n_estimators      = 500,           \n",
    "                              subsample         = 1.0, \n",
    "                              min_samples_split = 2,           \n",
    "                              min_samples_leaf  = 1,\n",
    "                              min_weight_fraction_leaf = 0.0, \n",
    "                              max_depth         = 3,           \n",
    "                              init              = None,      \n",
    "                              random_state      = None,        \n",
    "                              max_features      = None,      \n",
    "                              verbose           = 0,           \n",
    "                              max_leaf_nodes    = None,           \n",
    "                              warm_start        = False,       \n",
    "                              presort           = 'auto'),\n",
    "\n",
    "   StochasticGradientDescent(alpha         = 0.0001,  \n",
    "                             average       = False,     \n",
    "                             class_weight  = 'balanced', \n",
    "                             epsilon       = 0.1,  \n",
    "                             eta0          = 0.0,       \n",
    "                             fit_intercept = True, \n",
    "                             l1_ratio      = 0.15, \n",
    "                             learning_rate = 'optimal', \n",
    "                             loss          = 'hinge', \n",
    "                             n_iter        = 500,    \n",
    "                             n_jobs        = -1,        \n",
    "                             penalty       = 'l1',        \n",
    "                             power_t       = 0.5,  \n",
    "                             random_state  = None,      \n",
    "                             shuffle       = True, \n",
    "                             verbose       = 0,   \n",
    "                             warm_start    = False),\n",
    "    Classifier(layers=[\n",
    "        Layer(\"Rectifier\", units = 100),\n",
    "        Layer(\"Sigmoid\",   units = 100, weight_decay = 0.00001),\n",
    "        Layer(\"Softmax\")],\n",
    "                          warning       = None, \n",
    "                          weights       = None, \n",
    "                          random_state  = None, \n",
    "                          learning_rule = u'sgd', \n",
    "                          learning_rate = 0.001, \n",
    "                          learning_momentum= 0.9, \n",
    "                          regularize    = None, \n",
    "                          weight_decay  = None, \n",
    "                          dropout_rate  = None, \n",
    "                          batch_size    = 1, \n",
    "                          n_iter        = 100, \n",
    "                          n_stable      = 10, \n",
    "                          f_stable      = 0.001,\n",
    "                          valid_set     = None, \n",
    "                          valid_size    = 0.0,\n",
    "                          loss_type     = None,\n",
    "                          callback      = None, \n",
    "                          debug         = False, \n",
    "                          verbose       = None),\n",
    "    Classifier(layers=[\n",
    "        Layer(\"Rectifier\", units = 100),\n",
    "        Layer(\"Sigmoid\",   units = 100, weight_decay = 0.001),\n",
    "        Layer(\"Softmax\")],\n",
    "                          warning        = None, \n",
    "                          weights        = None, \n",
    "                          random_state   = None, \n",
    "                          learning_rule  = u'sgd', \n",
    "                          learning_rate  = 0.01, \n",
    "                          learning_momentum= 0.9, \n",
    "                          regularize     = None, \n",
    "                          weight_decay   = None, \n",
    "                          dropout_rate   = None, \n",
    "                          batch_size     = 1, \n",
    "                          n_iter         = 100, \n",
    "                          n_stable       = 10, \n",
    "                          f_stable       = 0.001,\n",
    "                          valid_set      = None, \n",
    "                          valid_size     = 0.0,\n",
    "                          loss_type      = None,\n",
    "                          callback       = None, \n",
    "                          debug          = False, \n",
    "                          verbose        = None),\n",
    "    Classifier(layers=[\n",
    "        Layer(\"Rectifier\", units = 100),\n",
    "        Layer(\"Sigmoid\",   units = 100, weight_decay = 0.01),\n",
    "        Layer(\"Softmax\")],\n",
    "                          warning         = None, \n",
    "                          weights         = None, \n",
    "                          random_state    = None, \n",
    "                          learning_rule   = u'sgd', \n",
    "                          learning_rate   = 0.09, \n",
    "                          learning_momentum = 0.05, \n",
    "                          regularize      = None,\n",
    "                          weight_decay    = None, \n",
    "                          dropout_rate    = None, \n",
    "                          batch_size      = 1, \n",
    "                          n_iter          = 100, \n",
    "                          n_stable        = 10, \n",
    "                          f_stable        = 0.001,\n",
    "                          valid_set       = None, \n",
    "                          valid_size      = 0.0,\n",
    "                          loss_type       = None,\n",
    "                          callback        = None, \n",
    "                          debug           = False, \n",
    "                          verbose         = None),\n",
    "    Classifier(layers=[\n",
    "        Layer(\"Rectifier\", units = 100),\n",
    "        Layer(\"Sigmoid\",   units = 100, weight_decay = 0.001),\n",
    "        Layer(\"Softmax\")],\n",
    "                          warning       = None, \n",
    "                          weights       = None, \n",
    "                          random_state  = None, \n",
    "                          learning_rule = u'sgd', \n",
    "                          learning_rate = 0.02, \n",
    "                          learning_momentum = 0.05, \n",
    "                          regularize    = None, \n",
    "                          weight_decay  = None, \n",
    "                          dropout_rate  = None, \n",
    "                          batch_size    = 1, \n",
    "                          n_iter        = 100, \n",
    "                          n_stable      = 10, \n",
    "                          f_stable      = 0.001,\n",
    "                          valid_set     = None, \n",
    "                          valid_size    = 0.0,\n",
    "                          loss_type     = None,\n",
    "                          callback      = None, \n",
    "                          debug         = False, \n",
    "                          verbose       = None)\n",
    "]\n",
    "\n",
    "\n",
    "weakNames       = ['Bagging', 'adaBoost']\n",
    "weakClassifiers = [BaggingClassifier(base_estimator = classifiers,  \n",
    "                                     n_estimators = 2),\n",
    "                   \n",
    "                   AdaBoostClassifier(base_estimator = classifiers,\n",
    "                                      n_estimators    = 2,\n",
    "                                      learning_rate   = 1.0,\n",
    "                                      random_state    = None)\n",
    "                  ]\n",
    "ValidationName          = ['ConfusionMatrix', \n",
    "                           'CrossValidation Score for n ks', 'Accuracy Score']\n",
    "\n",
    "ValidationMetricsRunAll = [#confusionMatrixPlot(target_test, target_predicted, target_names),\n",
    "                           #CvScores(target_predicted, features_train,  target_train, 2),\n",
    "                           #accScore(clf_y_train_pred, clf_y_test_pred)\n",
    "                          ]\n",
    "\n",
    "parameters = []\n",
    "clf = 0 \n",
    "import csv \n",
    "\n",
    "def strongLearners(names, classifier):\n",
    "    for name, clf,  in zip(names, classifiers):\n",
    "        print 'Model Parameters: {}'.format(clf), '\\n'\n",
    "        print 'Name: {}'.format(name), '\\n'\n",
    "        \n",
    "        workerFunction(clf, \n",
    "                       features_train, \n",
    "                       features_test, \n",
    "                       target_train, \n",
    "                       target_test)\n",
    "        \n",
    "        accScore(clf_y_train_pred, \n",
    "                 clf_y_test_pred)\n",
    "        \n",
    "        confusionMatrixPlot(target_test, \n",
    "                            target_predicted, \n",
    "                            target_names)\n",
    "        CvScores(clf, \n",
    "                 features_train,\n",
    "                 target_train, 7)\n",
    "        \n",
    "        #gridSearch(clf SVC_Param, 4)\n",
    "                \n",
    "        \n",
    "# weakClassifiers = []\n",
    "# for cls in classifiers:\n",
    "#     tmp = BaggingClassifier(base_estimator = cls, n_estimators = 2)\n",
    "#     workerFunction(clf, features_train, features_test, target_train, target_test)\n",
    "#     print tmp\n",
    "#     weakLearners.append(tmp)        \n",
    "  \n",
    "def weakLearners(weakNames, classifiers):\n",
    "    weakLearners = []\n",
    "    \"\"\"\n",
    "    Need to add error Error Exception for models that \n",
    "    cannot be Bagged or adaBoosted\n",
    "    \"\"\"\n",
    "    for names, clf in zip (weakNames, classifiers):\n",
    "        weakBagg = BaggingClassifier(base_estimator = clf,\n",
    "                                     n_estimators = 2\n",
    "                                    )\n",
    "        weakAdaB = AdaBoostClassifier(base_estimator  = clf,\n",
    "                                      n_estimators    = 2,\n",
    "                                      learning_rate   = 1.0,\n",
    "                                      random_state    = None\n",
    "                                     )\n",
    "    print 'Model Parameters: {}'.format(weakBagg), '\\n'\n",
    "    print 'Name: {}, Bagged'.format(names), '\\n'\n",
    "\n",
    "    workerFunction(weakBagg, \n",
    "                   features_train, \n",
    "                   features_test,\n",
    "                   target_train,\n",
    "                   target_test),\n",
    "    \n",
    "    accScore(clf_y_train_pred, \n",
    "             clf_y_test_pred),\n",
    "    \n",
    "    confusionMatrixPlot(target_test,\n",
    "                        target_predicted, \n",
    "                        target_names), \n",
    "    \n",
    "    CvScores(weakBagg, \n",
    "             features_train, \n",
    "             target_train, \n",
    "             7) \n",
    "    \n",
    "    print 'Model Parameters: {}'.format(weakAdaB), '\\n'\n",
    "    print 'Name: {}, Bagged'.format(names), '\\n'\n",
    "\n",
    "    workerFunction(weakAdaB, \n",
    "                   features_train, \n",
    "                   features_test,\n",
    "                   target_train,\n",
    "                   target_test)\n",
    "    \n",
    "    accScore(clf_y_train_pred, \n",
    "             clf_y_test_pred)\n",
    "    \n",
    "    confusionMatrixPlot(target_test,\n",
    "                        target_predicted, \n",
    "                        target_names)\n",
    "    \n",
    "    CvScores(weakAdaB, \n",
    "             features_train, \n",
    "             target_train, \n",
    "             7)\n",
    "\n",
    "    #weakLearners.append()\n",
    "        \n",
    "#########  NEED PARAMETER FUNCTION THAT ######### \n",
    "######## RUNS AFTER ALL OF THE BASE MODELS ######### \n",
    "\n",
    "def runModels():\n",
    "    allModels = [strongLearners(names, classifiers),\n",
    "                 weakLearners(weakNames, classifiers) ]\n",
    "    for all_ in allModels:\n",
    "        try:\n",
    "            funcs()\n",
    "        except:\n",
    "            break \n",
    "            \n",
    "            \n",
    "########## ########## ########## ########## ########## ########## \n",
    "   ##### Future Improvements / Implementations ########## #####\n",
    "########## ########## ########## ########## ########## ########## \n",
    "\n",
    "def ParametersFunk(): \n",
    "    \"\"\"\n",
    "    runs through various params, measures the result, then \n",
    "    Prints best to one csv, and others to a second.\n",
    "    \"\"\"\n",
    "\n",
    "# An example that I found on the internet that is applicable to my final product. \n",
    "# https://gist.github.com/miguelmalvarez/07e622357b089fee7f21\n",
    "\"\"\"\n",
    "Logging on event.\n",
    "\"\"\"\n",
    "def current_timestamp():\n",
    "    ts = time.time()\n",
    "    return datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def log_info(message):\n",
    "    ts = time.time()\n",
    "    logging.info(message + \" \" + current_timestamp())\n",
    "\n",
    "def init_logging(log_file_path):\n",
    "    logging.basicConfig(format='%(message)s', level=logging.INFO, filename=log_file_path)\n",
    "\n",
    "# Magic Sauce. \n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def candidate_families():\n",
    "    \n",
    "    candidates = [] \n",
    "    \"\"\"\n",
    "    This will house all models AND PARAMETERS that \n",
    "    will be ran throughout this problem space! \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Example: \n",
    "    \"\"\"\n",
    "    \n",
    "    candidates.append(['SVM', SVC(), svm_param ])\n",
    "    \"\"\" \n",
    "    Repeat process for all models\n",
    "    \"\"\"\n",
    "    candidates = pd.DataFrame(runModels)\n",
    "    return candidates\n",
    "    \n",
    "def bestModel(): \n",
    "    \n",
    "    def decideMetrics(clf):\n",
    "        \"\"\"\n",
    "        Run logic that pulls metrics that are most applicable \n",
    "        to the model that is currently being ran. \n",
    "        \"\"\"\n",
    "        # Pseudo Code \n",
    "        \"\"\"\n",
    "        ---------- Measuring of metrics ---------\n",
    "        If clf is equal to model:\n",
    "        Then run function associated with that model \n",
    "        ifelse second \n",
    "        ifelse third \n",
    "        ...\n",
    "        else\n",
    "        run workerFunction\n",
    "        \"\"\"\n",
    "def saveCSV():\n",
    "  \"\"\"\n",
    "  1. Prepare file for output dump. \n",
    "    *** Read in the Metrics that were used to validate each model, \n",
    "    *** Then print each Metric and append previous Metric in first column.\n",
    "  2. For each model ran, create new column (++1)\n",
    "    *** If model did not contain a Metric, then print, \"NaNa For This Model\"\n",
    "  \"\"\"\n",
    "\n",
    "# def dataPrep_Train():\n",
    "#     targetName = ('caption*you')  \n",
    "#     df = perpareTarget(df, targetName, 'ad.', 'nonad')\n",
    "#     scaling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    #dataPrep_Train() \n",
    "    #runModels()\n",
    "    strongLearners(names, classifiers)\n",
    "    #weakLearners(weakNames, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: KNeighborsClassifier(algorithm='brute', leaf_size=20, metric='cosine',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
      "           weights='uniform') \n",
      "\n",
      "Name: KNN \n",
      "\n",
      "train/test accuracies 0.850 / 0.925\n",
      "Confusion Matrix Plot:\n",
      "[[37  0]\n",
      " [ 3  0]]\n",
      "Accuracy Score:\n",
      "0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brandon.tomlin\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     0 = no       0.93      1.00      0.96        37\n",
      "    1 = yes       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.86      0.93      0.89        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37  0]\n",
      " [ 3  0]]\n",
      "Cross Validation Score for each K [ 0.1875      0.4375      0.71428571  0.71428571  0.35714286  0.78571429\n",
      "  0.42857143] \n",
      "\n",
      "ROC AUC: 0.52 (+/- 0.21)\n",
      "Model Parameters: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=1, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best') \n",
      "\n",
      "Name: DecisionTree \n",
      "\n",
      "train/test accuracies 0.850 / 0.925\n",
      "Confusion Matrix Plot:\n",
      "[[37  0]\n",
      " [ 3  0]]\n",
      "Accuracy Score:\n",
      "0.925\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     0 = no       0.93      1.00      0.96        37\n",
      "    1 = yes       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.86      0.93      0.89        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37  0]\n",
      " [ 3  0]]\n",
      "Cross Validation Score for each K [ 0.4375      0.3125      0.35714286  0.71428571  0.71428571  0.64285714\n",
      "  0.35714286] \n",
      "\n",
      "ROC AUC: 0.51 (+/- 0.17)\n",
      "Model Parameters: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=1, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best') \n",
      "\n",
      "Name: DecisionTree1 \n",
      "\n",
      "train/test accuracies 0.850 / 0.925\n",
      "Confusion Matrix Plot:\n",
      "[[37  0]\n",
      " [ 3  0]]\n",
      "Accuracy Score:\n",
      "0.925\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     0 = no       0.93      1.00      0.96        37\n",
      "    1 = yes       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.86      0.93      0.89        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37  0]\n",
      " [ 3  0]]\n",
      "Cross Validation Score for each K [ 0.4375      0.3125      0.71428571  0.          0.71428571  0.64285714\n",
      "  0.35714286] \n",
      "\n",
      "ROC AUC: 0.45 (+/- 0.24)\n",
      "Model Parameters: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "            oob_score=False, random_state=True, verbose=0,\n",
      "            warm_start=False) \n",
      "\n",
      "Name: RandomForest \n",
      "\n",
      "train/test accuracies 0.983 / 0.950\n",
      "Confusion Matrix Plot:\n",
      "[[37  0]\n",
      " [ 2  1]]\n",
      "Accuracy Score:\n",
      "0.95\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     0 = no       0.95      1.00      0.97        37\n",
      "    1 = yes       1.00      0.33      0.50         3\n",
      "\n",
      "avg / total       0.95      0.95      0.94        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37  0]\n",
      " [ 2  1]]\n",
      "Cross Validation Score for each K [ 0.4375      0.625       0.57142857  0.14285714  1.          1.          0.        ] \n",
      "\n",
      "ROC AUC: 0.54 (+/- 0.36)\n",
      "Model Parameters: LinearSVC(C=1.0, class_weight='auto', dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0) \n",
      "\n",
      "Name: Support Vector Machine (Linear) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brandon.tomlin\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\utils\\class_weight.py:62: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n",
      "  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test accuracies 0.683 / 0.825\n",
      "Confusion Matrix Plot:\n",
      "[[31  6]\n",
      " [ 1  2]]\n",
      "Accuracy Score:\n",
      "0.825\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     0 = no       0.97      0.84      0.90        37\n",
      "    1 = yes       0.25      0.67      0.36         3\n",
      "\n",
      "avg / total       0.91      0.82      0.86        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[31  6]\n",
      " [ 1  2]]\n",
      "Cross Validation Score for each K [ 0.4375      0.6875      0.85714286  0.          1.          0.85714286\n",
      "  0.28571429] \n",
      "\n",
      "ROC AUC: 0.59 (+/- 0.33)\n",
      "Model Parameters: ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False) \n",
      "\n",
      "Name: ExtraTrees \n",
      "\n",
      "train/test accuracies 0.983 / 0.950\n",
      "Confusion Matrix Plot:\n",
      "[[37  0]\n",
      " [ 2  1]]\n",
      "Accuracy Score:\n",
      "0.95\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     0 = no       0.95      1.00      0.97        37\n",
      "    1 = yes       1.00      0.33      0.50         3\n",
      "\n",
      "avg / total       0.95      0.95      0.94        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37  0]\n",
      " [ 2  1]]\n",
      "Cross Validation Score for each K [ 0.40625     0.59375     0.57142857  0.35714286  0.5         1.\n",
      "  0.21428571] \n",
      "\n",
      "ROC AUC: 0.52 (+/- 0.23)\n",
      "Model Parameters: GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False) \n",
      "\n",
      "Name: GradientBoosting \n",
      "\n",
      "train/test accuracies 0.983 / 0.875\n",
      "Confusion Matrix Plot:\n",
      "[[34  3]\n",
      " [ 2  1]]\n",
      "Accuracy Score:\n",
      "0.875\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     0 = no       0.94      0.92      0.93        37\n",
      "    1 = yes       0.25      0.33      0.29         3\n",
      "\n",
      "avg / total       0.89      0.88      0.88        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34  3]\n",
      " [ 2  1]]\n",
      "Cross Validation Score for each K [ 0.5625      0.6875      0.57142857  0.28571429  0.57142857  1.\n",
      "  0.57142857] \n",
      "\n",
      "ROC AUC: 0.61 (+/- 0.20)\n",
      "Model Parameters: SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',\n",
      "       epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=500, n_jobs=-1,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False) \n",
      "\n",
      "Name: StochasticGradientDescent \n",
      "\n",
      "train/test accuracies 0.250 / 0.200\n",
      "Confusion Matrix Plot:\n",
      "[[ 5 32]\n",
      " [ 0  3]]\n",
      "Accuracy Score:\n",
      "0.2\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     0 = no       1.00      0.14      0.24        37\n",
      "    1 = yes       0.09      1.00      0.16         3\n",
      "\n",
      "avg / total       0.93      0.20      0.23        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 5 32]\n",
      " [ 0  3]]\n",
      "Cross Validation Score for each K [ 0.4375      0.5         0.42857143  0.          0.71428571  0.85714286\n",
      "  0.14285714] \n",
      "\n",
      "ROC AUC: 0.44 (+/- 0.28)\n",
      "Model Parameters: Classifier(batch_size=1, callback=None, debug=False, dropout_rate=None,\n",
      "      f_stable=0.001,\n",
      "      hidden0=<sknn.nn.Layer `Rectifier`: units=100, name=u'hidden0', frozen=False>,\n",
      "      hidden1=<sknn.nn.Layer `Sigmoid`: units=100, name=u'hidden1', frozen=False, weight_decay=1e-05>,\n",
      "      layers=[<sknn.nn.Layer `Rectifier`: units=100, name=u'hidden0', frozen=False>, <sknn.nn.Layer `Sigmoid`: units=100, name=u'hidden1', frozen=False, weight_decay=1e-05>, <sknn.nn.Layer `Softmax`: name=u'output', frozen=False>],\n",
      "      learning_momentum=0.9, learning_rate=0.001, learning_rule=u'sgd',\n",
      "      loss_type=None, n_iter=100, n_stable=10,\n",
      "      output=<sknn.nn.Layer `Softmax`: name=u'output', frozen=False>,\n",
      "      random_state=None, regularize=None, valid_set=None, valid_size=0.0,\n",
      "      verbose=None, warning=None, weight_decay=None, weights=None) \n",
      "\n",
      "Name: MLP \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\brandon.tomlin\\appdata\\local\\continuum\\anaconda\\src\\theano\\theano\\tensor\\signal\\downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test accuracies 0.850 / 0.925\n",
      "Confusion Matrix Plot:\n",
      "[[37  0]\n",
      " [ 3  0]]\n",
      "Accuracy Score:\n",
      "0.925\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     0 = no       0.93      1.00      0.96        37\n",
      "    1 = yes       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.86      0.93      0.89        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37  0]\n",
      " [ 3  0]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = SVC(C=10.0, \n",
    "          kernel='rbf', \n",
    "          gamma=0.1, \n",
    "          decision_function_shape='ovr')\n",
    "\n",
    "kernel_svm = Pipeline([('std', StandardScaler()), \n",
    "                       ('svc', clf)])\n",
    "\n",
    "# gridsearch setup\n",
    "param_grid = [\n",
    "  {'svc__C': [0.00001, 0.0001, 0.001, 0.01, .1, 1, 10, 100, 1000], \n",
    "   'svc__gamma': [0.00001, 0.0001, 0.001, 0.01, 0.1], \n",
    "   'svc__kernel': ['rbf', 'linear']},\n",
    " ]\n",
    "\n",
    "gs = GridSearchCV(estimator=kernel_svm, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  n_jobs=-1, \n",
    "                  cv=7, \n",
    "                  verbose=1, \n",
    "                  refit=True,\n",
    "                  pre_dispatch='2*n_jobs')\n",
    "\n",
    "gs.fit(features_train, target_train)\n",
    "\n",
    "print('Best GS Score %.2f' % gs.best_score_)\n",
    "print('best GS Params %s' % gs.best_params_)\n",
    "\n",
    "\n",
    "# prediction on the training set\n",
    "accScore(clf_y_train_pred, \n",
    "                 clf_y_test_pred)\n",
    "confusionMatrixPlot(target_test, \n",
    "                    target_predicted, \n",
    "                    target_names)\n",
    "CvScores(clf, \n",
    "         features_train,\n",
    "         target_train, 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
